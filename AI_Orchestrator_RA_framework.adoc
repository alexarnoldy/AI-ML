:SUSEDocType: Reference Configuration
:SUSEDocTypeNoSpaces: Reference-Configuration
:Author: Alex Arnoldy
:AuthorEMail: Alex.Arnoldy@SUSE.com

:CompanyName: SUSE
:ProductName: AI Orchestrator
:ProductNameCaaSP: CaaS Platform
:ProductNameSES: Enterprise Storage

:IHVPartner: Dev_Null
:IHVNetwork: DN-100-Network
:IHVPlatform: DN-100-Compute 
:IHVSESPlatform: DN-100-Storage 
:IHVPlatformComposer: NotApplicable
:IHVPlatformComposerTech: NotApplicable
:IHVPlatformImager: NotApplicable
:IHVPlatformModel: DN-100-compute
:IHVSESPlatformModel: DN-100-storage
:IHVPlatformBMC: DN-BMC
:IHVNetworkModel: DN-100-network
:IHVNetworkSpeed: 25GbE

:MarketCategory: Artifical Intelligence / Machine Learning
:MarketCategoryAbbreviation: AI/ML

= {susedoctype}: {CompanyName} {ProductName} with {IHVPartner} {IHVPlatform}

{Author}, {CompanyName} < {AuthorEMail} >

== Executive Summary
This reference configuration is designed to help organizations understand and take full advantage of the SUSE {ProductName} to manage workloads run on an {MarketCategory} frameworks. While SUSE {productname} can manage multiple {marketcategoryabbreviation} frameworks, both on-premise and the cloud. This document focuses on using {productname} to manage Kubeflow running on SUSE {productnamecaasp}. This document provides an archtitectural overview and value anaylsis for using {productname} to manage the work of adapting and running ML analysis models production-ready Kubeflow Pipelines. Finally, it provides common design considerations, implementation suggestions and best practices.

For most enterprise-level businesses, the demand for {marketcategoryabbreviation} projects is increasing rapidly. All too often these projects start in a piecemeal fashion requiring excessive custom, non-reusble steps as they progress. Unfortunately, many of these projects are doomed to a painful path to inevitable failure as their highly skilled proprietors become mired in cumbersome tasks of creating, and then maintaining, the custom infrastructure for a single project. This well documented problem as been come to be known in the {marketcategoryabbreviation} space as the https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf[Hidden Technical Debt of Machine Learning Systems]. Fortunately, {marketcategoryabbreviation} professionals already have a well defined decipline to follow in the world of DevOps. As in the DevOps world, one of the most important keys to success in many {marketcategory} projects is leveraging technologies that minimize manual intervention and maximize repeatability.

Implementing an intelligent, automation-focused management layer, such as {productname} enables you to transform your {marketcategoryabbreviation} from requiring an almost impossible to define number of manual steps into a clearly defined effort with repeatable steps and milestones. In addition, the SUSE {productname} enables AI Operations teams to track and influence the resources expended on {marketcategoryabbreviation} projects and even individual analysis runs. 

== Target Audience
This document is intended for IT decision makers, architects, data scientists and AI operators who are implementing an ever increasing number of {marketcategoryabbreviation} projects; and struggling under the friction of bringing {marketcategoryabbreviation} models from design to revenue generating production.  You should be familiar with the traditional IT infrastructure pillars—networking, computing and storage—along with key concepts in the field of developing, testing, and deploying {marketcategory} predictive models.

== Solution Overview
When SUSE set out to create a new, indispensible {marketcategoryabbreviation} tool for its customers, we were certain that we didn't want to create just another framework/training/analysis tool. What the wild west of {marketcategoryabbreviation} needs more than anything else is a tool that returns control of the tools to the AI consumer and improves the chances of success for every AI/ML project.

From the outset, in creating the SUSE {productname} we followed several primary guidelines: 
* Enable Data Scientists to stay focused on the creative end of data model design and development
* Significantly improve the visibility and control for the AI Operations team over the AI Platforms and expendatures
* Improve cross team visibility and enable new avenues of collaboration between Data Scientists, AI Engineers, and AI Operators
* Enable model pipeline execution on virtually any platform, from a single workstation, to Kubeflow, to any public cloud provider; all without having to rewrite any part of the model or pipeline definition.


As an {marketcategoryabbreviation} pipeline development and management tool, the SUSE {productname} has the ability to manage operations on multiple platforms. With its large community of developers and a plethora of features and capabilities, the initial focus of the {productname} is Kubeflow running on SUSE {productnamecaasp}. 


== Solution Components

=== Facility
SUSE {productname} is a fully containerized Linux application and can run on any platform that is configured to run Linux containers. While a small data science team may choose to run the {productname} on a workstation, most customers will likely prefer to run it within a highly resilient container ochestrator, such as SUSE {productnamecaasp}.

=== {marketcategoryabbreviation} Platform
Kubeflow is a open-source machine learning platform that enables running machine learning projects on a highly resilient and scalable Kubernetes platform. Kubeflow is actually a collection of nearly a dozen different {marketcategoryabbreviation} frameworks and tools including Jupyter Notebook, TensorFlow, and Pytorch. Being a Kubernetes application suite, the installation and initial configuration is deceptively simple; though the resultant set of applications is one of the most powerful in the industry.

=== {productnamecaasp} Platform
{CompanyName} {ProductName} is an enterprise-class container management solution that enables IT and DevOps professionals to more easily deploy, manage, and scale container-based applications and services. It includes Kubernetes to automate lifecycle management of modern applications and surrounding technologies that enrich Kubernetes, built from industry-leading technologies, and delivered as a complete package — with everything you need to quickly offer container services. As a result, enterprises that use {CompanyName} {ProductName} can reduce application delivery cycle times and improve business agility.

{CompanyName} is focused on delivering an exceptional operator experience with {CompanyName} {ProductName}. With deep competencies in infrastructure, systems, process integration, platform security, lifecycle management and enterprise-grade support, {CompanyName} aims to ensure that IT operations teams can deliver the power of Kubernetes to their users quickly, securely and efficiently.

With {CompanyName} {ProductName} you can:
* Achieve faster time to value with an enterprise-ready container management platform
* Simplify management and control of your container platform with efficient installation, easy scaling, and update automation. 
* Maximize return on your investment, with a flexible container services solution for today and tomorrow.

=== Compute
While the needs of the SUSE {productname} are fairly low, Kubeflow is powerful {marketcategoryabbreviation} platform that needs significant resources to reach its maximum potential. The {productname} can be deployed either on an {marketcategoryabbreviation} workstation or a Kubernetes cluster. For the purposes of this document, the {productname} was deployed in the same Kubernetes cluster as Kubeflow and thus its additional resource consumption is negliable.

Most {marketcategoryabbreviation} workloads follow a fairly predictable set of rules in terms of compute resource needs. Training and validation workloads take full advantage of Enterprise class GPUs. However, the needs of purely serving workloads can often be met with Data Center CPUs with an above average number of cores per processor. 

As with most every workload on modern compute infrastructure, the most constraining factor for {marketcategory} applications is the quantity of memory available to them. While one approach might be to maximize the amount of memory per application server, a moderating consideration to this would be to balance the amount of memory per physical server with the number of application servers in the cluster. One must always plan for the impact of the failure of a single control plane or application server. Ideally, the failure of a single application server shouldn't impact more than 10% - 15% of the total workload.

For this solution, the {ihvpartner} {ihvplatform} servers were ideal in terms of CPU, memory, and network capabilities. 

include::https://github.com/alexarnoldy/_includes/blob/master/IHV/{IHVPartner}/{IHVPlatform}.adoc[]

<<<<IHV Partner: Include a short marketing description (not a BOM) of the compute servers used.>>>>

=== Storage
Many of the Kubeflow applications leverage a large amount of highly available, highly resilient, persistent storage.

SUSE {productnamecaasp} can access both network block and object storage via the modular Container-Storage-Interface (CSI) technologies. A customer can leverage an existing storage platform, that serve multiple workloads in the data center, and thus maximize the value of existing assets. Alternatively, a dedicated storage solution can be deployed to ensure maximum performance and capacity is available for the company's {marketcategoryabbreviation} projects.

One of the most popular storage solutions available for containerized workloads is SUSE {productnameses}, which is a Ceph based, software-defined storage infrastructure. In addition to versatility, SUSE {productnameses} can scale into the dozens of petabytes while scaling linearly in performance.

When developing a Ceph based storage solution, balance is key. To reach maximum performance and resiliency, one must balancing the CPUs, memory, and network capacity with that of the storage drives (as well as SSD/NVMe accelerators) per node. As well, the capacity of each node should be in balance with the number number of nodes in the cluster. Just as with the Kubernetes worker nodes, the impact of a single node failure must not severely impact the performance or availability of the entire cluster.

While there are a myriad of options available, we found that the {ihvpartner} {ihvsesplatform} {ihvsesplatformmodel} servers were outstanding for this role.

include::https://github.com/alexarnoldy/_includes/blob/master/IHV/{IHVPartner}/{IHVSESPlatform}.adoc[]

<<<<IHV Partner: Include a short marketing description (not a BOM) of the storage servers used.>>>>

NOTE: Such integrations, with solutions like {CompanyName} {ProductNameSES}, are detailed in other reference documents.

=== Network

While the network for this solution does not need to be complex; there are a few, critical features to consider when designing it.

A network load balancer is required for high availability of the {productnamecaasp} master nodes. An additional load balancer should be provided for the application workload running on the {productnamecaasp} cluster. These can be hardware or software based, but are outside the scope of this document.

As both SUSE {productnamecaasp} and {productnameses} are scale out solutions, network resiliency is a base expectation. Any single point of failure in the network design could lead to performance penalties up to in including significant data loss.

VLAN isolation should be used to separate, at a minimum, compute traffic from backend storage traffic. Further isolation would provide a minimum level of security isolation while maximizing network switch performance. Additional VLANs could be used to segregate external traffic reaching the load balancer(s), {productnamecaasp} traffic, and {productnameses} Storage-Backend.

It is often said that the chain is only as strong as its weakest link. That is certainly true if the weakest link is the network. It makes no sense connecting highly powerful servers with unreliable or underperforming network hardware. For this reason we were pleased to be able to leverage the speed and reliability of the {ihvpartner} {ihvnetworkmodel} {ihvnetworkspeed} switches.

include::https://github.com/alexarnoldy/_includes/blob/master/IHV/{IHVPartner}/{IHVNetwork}.adoc[]

<<<<IHV Partner: Include a short marketing description (not a BOM) of the network switches used.>>>>

== Solution Details
This document focuses on the advanages of an {marketcategoryabbreviation} solution stack centered around the {CompanyName} {ProductName}.

The Data Scientist is the most important element in any Artificial Intelligence inititive. The more time she can spend creating, analyzing, and refining her data models more chances of success the project will have. Unfortunately, the reality is that there are several other elements of the project that will siphone off her time. 

Often, Data Scientists prefer to build, and initally test, their models on their own local workstation. This gives them full control of the AI platform and tends to work well for the initial process of model development. However, to move the model into the next phases of development, the work needs to continue with more computing power and much larger data sets. In addition, many data valiation/preprocessing and model training/tuning steps must be taken as the model (and even multiple variations of the model) continue through development. 

Unfortunately, far too often a data science team will run many of these steps manually, or with custom scripts and one-off  "glue-code" tools. In the end, they will spend more time cultivating a custom ecosphere for a single project than developing the model. While most every data scientist understands the benefit of leveraging AI pipeline orchestrators such as Apache Airflow and Kubeflow, it also represents even more work to learn the needed SDKs and code the meta language to convert all of the data processing and model valdation/tuning steps into a pipeline. This is where the SUSE {productname} fulfills its promise of keeping data scientists focused on creative endeavors. By analyzing the ML model, the SUSE {productname} can determine the flow of tasks required to develop a funcioning pipeline for the model development. During this analysis, the {productname} will develop and display a directed acyclic graph the tasks and even show the progress of an analytic run of the model through the graph.

The SUSE {productname} wasn't just created to help data scientists, but rather to empower the entire Enterprise {marketcategoryabbreviation} business unit. This occurs almost organically when the different {marketcategoryabbreviation} teams gain new visibility into the needs of their counterparts. The data science teams will be able to see the capabilities of the {marketcategoryabbreviation} platforms and work directly with the AI operations teams to match the operational costs and capabilities of a platform with the needs of the training/tuning run need at the moment. The AI operations team will also be able to advance the model runs many times faster with the additional visibility they gain into the full pipeline execution. They will be able to feed back errors and exceptions the occur, in real time, to the data science team. Since the {productname} leverages any Git compatible repository for versioning control, the data scientist can quickly fix errors and commit a new version, which can automatically trigger a repeat of the same run, based on the updated model.

Since the SUSE {productname} pipeline definiations are platform agnostic, the exact same run can be moved to another platform if the teams decide that the current platform doesn't meet their needs in terms of capabilities or cost. 

== Conclusion
The {marketcategory} market is growing at a break-neck pace. Most {marketcategoryabbreviation} teams aren't looking for another analytics platform, workspace or training tool. What these teams need is a tool that empowers them to focus on the work they do best and colaborate in ways they may not have even imagined possible. Data scientists don't want to invest their time in creating "glue-code" for every project. They would rather rely on automated, standardized procedures that allow them to easily advance their projects from design to production; then allow them to quickly move on to the next project. AI operations teams don't want to be stuck between demands for the highest performing platforms and spiraling operations costs. They'd rather colaborate with the data science team to give them to most appropriate, and cost effective, platform for the needs and priorities of the project. SUSE {productname} opens so many opportunities for invovative, fast moving {marketcategoryabbreviation} to stay focused on their specialties, while cutting weeks and months off of their projects. Obviously, having standardized tools that work across the enterprise makes teams more effective, satisfied, and ensures new data scientists and AI operators will become productive much faster.


== Resources and additional links

include::https://github.com/alexarnoldy/_includes/blob/master/IHV/{IHVPartner}/{susedoctypenospaces}-{CompanyName}-{ProductName}-BOM.adoc[]

<<<<IHV Partner: Provide high level product BOMs for Compute, Storage, and Network devices; as well as any web links.>>>>